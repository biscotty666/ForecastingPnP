---
title: "Chapter 5 The forecaster's toolbox Part 1"
output:
  html_document:
    toc: true
    df_print: paged
  github_document:
    toc: true
---

The feasts package includes functions for computing FEatures And Statistics from Time Series.

```{r}
library(fpp3)
library(feasts)
```

# 5.1 A tidy forecasting workflow

To illustrate the process, we will fit linear trend models to national GDP data stored in `global_economy`.

## Data preparation (tidy)

The first step in forecasting is to prepare data in the correct format. This process may involve loading in data, identifying missing values, filtering the time series, and other pre-processing tasks. The functionality provided by `tsibble` and other packages in the `tidyverse` substantially simplifies this step.

If we want to model GDP per capita over time we must first compute the variable

```{r}
gdppc <- global_economy |>
  mutate(GDP_per_capita = GDP / Population)
gdppc
```

## Visualize the data

Let's look at the data for one country

```{r}
gdppc |>
  filter(Country == "Sweden") |>
  autoplot(GDP_per_capita) +
  labs(y = "$US", title = "GDP per capita for Sweden")
```

## Specify a model

There are many different time series models that can be used for forecasting, and much of this book is dedicated to describing various models. Specifying an appropriate model for the data is essential for producing appropriate forecasts.

Models in fable are specified using model functions, which each use a formula (y ~ x) interface. The response variable(s) are specified on the left of the formula, and the structure of the model is written on the right.

For example, a linear trend model (to be discussed in Chapter 7) for GDP per capita can be specified with

```{r}
TSLM(GDP_per_capita ~ trend())
```

In this case the model function is `TSLM()` (time series linear model), the response variable is `GDP_per_capita` and it is being modelled using `trend()` (a “special” function specifying a linear trend when it is used within `TSLM()`). We will be taking a closer look at how each model can be specified in their respective sections.

The special functions used to define the model’s structure vary between models (as each model can support different structures). The “Specials” section of the documentation for each model function lists these special functions and how they can be used.

The left side of the formula also supports the transformations discussed in Section 3.1, which can be useful in simplifying the time series patterns or constraining the forecasts to be between specific values (see Section 13.3).

## Train the model (estimate)

Once an appropriate model is specified, we next train the model on some data. One or more model specifications can be estimated using the model() function.

```{r}
fit <- gdppc |>
  model(trend_model = TSLM(GDP_per_capita ~ trend()))
fit
```

This fits a linear trend model to the GDP per capita data for each combination of key variables in the `tsibble`. In this example, it will fit a model to each of the 263 countries in the dataset. The resulting object is a model table or a “**mable**”.

## Evaluate model performance

Once a model has been fitted, it is important to check how well it has performed on the data. There are several diagnostic tools available to check model behaviour, and also accuracy measures that allow one model to be compared against another. Sections 5.8 and 5.9 go into further details.

## Produce forecasts

With an appropriate model specified, estimated and checked, it is time to produce the forecasts using forecast(). The easiest way to use this function is by specifying the number of future observations to forecast. For example, forecasts for the next 10 observations can be generated using h = 10. We can also use natural language; e.g., h = "2 years" can be used to predict two years into the future.

In other situations, it may be more convenient to provide a dataset of future time periods to forecast. This is commonly required when your model uses additional information from the data, such as exogenous regressors. Additional data required by the model can be included in the dataset of observations to forecast.

```{r}
fit |> forecast(h = "3 years")
```

This is a forecast table, or “**fable**”. Each row corresponds to one forecast period for each country. The `GDP_per_capita` column contains the **forecast distribution**, while the `.mean` column contains the **point forecast**. The point forecast is the mean (or average) of the forecast distribution.

The forecasts can be plotted along with the historical data using autoplot() as follows.

```{r}
fit |>
  forecast(h = "3 years") |>
  filter(Country == "Sweden") |>
  autoplot(gdppc) +
  labs(y = "$US", title = "GDP per capita for Sweden")
```

# 5.2 Some simple forecasting methods

Some forecasting methods are extremely simple and surprisingly effective. We will use four simple forecasting methods as benchmarks throughout this book. 

To illustrate them, we will use quarterly Australian clay brick production between 1970 and 2004.

The `filter_index()` function is a convenient shorthand for extracting a section of a time series.

```{r}
bricks <- aus_production |>
  filter_index("1970 Q1" ~ "2004 Q4") |>
  select(Bricks)
bricks
```

## Mean method

Here, the forecasts of all future values are equal to the average (or “mean”) of the historical data. If we let the historical data be denoted by $y_1,\dots,y_T$, then we can write the forecasts as

$$\hat{y}_{T+h|T}=\bar{y}=(y_1+\dots+y_T)/T$$

The notation $\hat{y}_{T+h|T}$ is a short-hand for the estimate of $y_{T+h}$ based on the data $y_1,\dots,y_T$

```{r}
bricks |> model(MEAN(Bricks))
```

## Naive method

All forecasts set to the value of the last observation.

$$\hat{y}_{T+h|T} = y_{T}$$
 
 This method works well for many economic/financial forecasts.
 
```{r}
bricks |> model(NAIVE(Bricks))
```
 
## Seasonal naive method

A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season (e.g., the same month of the previous year). Formally, the forecast for time $T+h$ is written as

$$\hat{y}_{T+h|T} = y_{T+h-m(k+1)}$$

where $m=$ the seasonal period, and $k$ is the integer part of $(h−1)/m$ (i.e., the number of complete years in the forecast period prior to time $T+h$). This looks more complicated than it really is. For example, with monthly data, the forecast for all future February values is equal to the last observed February value. With quarterly data, the forecast of all future Q2 values is equal to the last observed Q2 value (where Q2 means the second quarter). Similar rules apply for other months and quarters, and for other seasonal periods.

```{r}
bricks |> model(SNAIVE(Bricks ~ lag("year")))
```

The `lag()` function is optional here as bricks is quarterly data and so a seasonal naïve method will need a one-year lag. However, for some time series there is more than one seasonal period, and then the required lag must be specified.



## Drift method

A variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the **drift**) is set to be the average change seen in the historical data. Thus the forecast for time $T+h$ is given by

$$\hat{y}_{T+h|T} = y_{T} + \frac{h}{T-1}\sum_{t=2}^T (y_{t}-y_{t-1}) = y_{T} + h \left( \frac{y_{T} -y_{1}}{T-1}\right)$$

This is equivalent to drawing a line between the first and last observations, and extrapolating it into the future.

```{r}
bricks |> model(RW(Bricks ~ drift()))
```

## Example: Australian quarterly beer production

The first three methods applied to Australian quarterly beer production from 1992 to 2006, with the forecasts compared against actual values in the next 3.5 years.

```{r}
# Set training data from 1992 to 2006
train <- aus_production |>
  filter_index("1992 Q1" ~ "2006 Q4")
# Fit the models
beer_fit <- train |>
  model(
    Mean = MEAN(Beer),
    `Naïve` = NAIVE(Beer),
    `Seasonal naïve` = SNAIVE(Beer)
  )
# Generate forecasts for 14 quarters
beer_fc <- beer_fit |> forecast(h = 14)
# Plot forecasts against actual values
beer_fc |>
  autoplot(train, level = NULL) +
  autolayer(
    filter_index(aus_production, "2007 Q1" ~ .),
    colour = "black"
  ) +
  labs(
    y = "Megalitres",
    title = "Forecasts for quarterly beer production"
  ) +
  guides(colour = guide_legend(title = "Forecast"))
```
 
In this case, only the seasonal naïve forecasts are close to the observed values from 2007 onwards

## Example: Google's daily closing stock price

The non-seasonal methods are applied to Google’s daily closing stock price in 2015, and used to forecast one month ahead. Because stock prices are not observed every day, we first set up a **new time index based on the trading days rather than calendar days**.

### Re-index based on trading days

```{r}
google_stock <- gafa_stock |>
  filter(Symbol == "GOOG", year(Date) >= 2015) |>
  mutate(day = row_number()) |>
  update_tsibble(index = day, regular = TRUE)
google_stock
```

### Filter the year of interest

```{r}
google_2015 <- google_stock |> filter(year(Date) == 2015)
google_2015
```

### Fit the models

```{r}
google_fit <- google_2015 |>
  model(
    Mean = MEAN(Close),
    `Naïve` = NAIVE(Close),
    Drift = NAIVE(Close ~ drift())
  )
google_fit
```

### Produce forecasts for the trading days in January 2016

```{r}
google_jan_2016 <- google_stock |>
  filter(yearmonth(Date) == yearmonth("2016 Jan"))
google_fc <- google_fit |>
  forecast(new_data = google_jan_2016)
```

### Plot the forecasts

```{r}
google_fc |>
  autoplot(google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, colour = "black") +
  labs(y = "$US",
       title = "Google daily closing stock prices",
       subtitle = "(Jan 2015 - Jan 2016") +
  guides(colour = guide_legend(title = "Forecast"))
```

Sometimes one of these simple methods will be the best forecasting method available; but in many cases, these methods will serve as **benchmarks** rather than the method of choice. That is, any forecasting methods we develop will be compared to these simple methods to ensure that the new method is better than these simple alternatives. If not, the new method is not worth considering.

# 5.3 Fitted values and residuals

## Fitted values

Each observation in a time series can be forecast using all previous observations. We call these **fitted values** and they are denoted by $\hat{y}_{t|t−1}$, meaning the forecast of $y_t$ based on observations $y_1,\dots,y_{t−1}$. We use these so often, we sometimes drop part of the subscript and just write $\hat{y}_t$ instead of $\hat{y}_{t|t−1}$. **Fitted values almost always involve one-step forecasts** (but see Section 13.8).

Actually, **fitted values are often not true forecasts** because any parameters involved in the forecasting method are estimated using all available observations in the time series, including future observations. For example, if we use the mean method, the fitted values are given by

$$\hat{y}_t = \hat{c}$$

where $\hat{c}$ is the average computed over all available observations, **including those at times after $t$**. Similarly, for the drift method, the drift parameter is estimated using all available observations. In this case, the fitted values are given by

$$\hat{y}_t = y_{t-1} + \hat{c}$$

where $\hat{c} = (y_T-y_1)/(T-1)$.  In both cases, there is a parameter to be estimated from the data. The “hat” above the $c$
reminds us that this is an estimate. When the estimate of $c$ involves observations after time $t$, the fitted values are not true forecasts. On the other hand, naïve or seasonal naïve forecasts do not involve any parameters, and so fitted values are true forecasts in such cases.

## Residuals

The “residuals” in a time series model are what is left over after fitting a model. The residuals are equal to the difference between the observations and the corresponding fitted values:

$$e_{t} = y_{t}-\hat{y}_{t}.$$

If a transformation has been used in the model, then it is often useful to look at residuals on the transformed scale. We call these “**innovation residuals**”. For example, suppose we modelled the logarithms of the data, $w_t=log(y_t)$. Then the innovation residuals are given by $w_t−\hat{w}_t$ whereas the regular residuals are given by $y_t−\hat{y}_t$. (See Section 5.6 for how to use transformations when forecasting.) If no transformation has been used then the innovation residuals are identical to the regular residuals, and in such cases we will simply call them “residuals”.

The fitted values and residuals from a model can be obtained using the **`augment()`** function. In the beer production example in Section 5.2, we saved the fitted models as beer_fit. So we can simply apply augment() to this object to compute the fitted values and residuals for all models.

```{r}
augment(beer_fit)
```

There are three new columns added to the original data:

- .fitted contains the fitted values;
- .resid contains the residuals;
- .innov contains the “innovation residuals” which, in this case, are identical to the regular residuals.

Residuals are useful in checking whether a model has adequately captured the information in the data. For this purpose, we use innovation residuals.

**If patterns are observable in the innovation residuals, the model can probably be improved**. We will look at some tools for exploring patterns in residuals in the next section.

# 5.4 Residual diagnostics

A good forecasting method will yield innovation residuals with the following **essential properties**:

1. **The innovation residuals are uncorrelated**. If there are correlations between innovation residuals, then there is information left in the residuals which should be used in computing forecasts.
2. **The innovation residuals have zero mean**. If they have a mean other than zero, then the forecasts are biased.

Any forecasting method that does not satisfy these properties can be improved. However, that does not mean that forecasting methods that satisfy these properties cannot be improved. It is possible to have several different forecasting methods for the same data set, all of which satisfy these properties. **Checking these properties is important in order to see whether a method is using all of the available information, but it is not a good way to select a forecasting method**.

If either of these properties is not satisfied, then the forecasting method can be modified to give better forecasts. Adjusting for bias is easy: if the residuals have mean $m$, then simply subtract $m$ from all forecasts and the bias problem is solved. Fixing the correlation problem is harder, and we will not address it until Chapter 10.

In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties.

1. The innovation residuals have constant variance. This is known as “homoscedasticity”.
2. The innovation residuals are normally distributed.

These two properties make the calculation of prediction intervals easier (see Section 5.5 for an example). However, a forecasting method that does not satisfy these properties cannot necessarily be improved. Sometimes applying a Box-Cox transformation may assist with these properties, but otherwise there is usually little that you can do to ensure that your innovation residuals have constant variance and a normal distribution. Instead, an alternative approach to obtaining prediction intervals is necessary. We will show how to deal with non-normal innovation residuals in Section 5.5.

## Example: Forecasting Google daily closing stock prices

We will continue with the Google daily closing stock price example from Section 5.2. **For stock market prices and indexes, the best forecasting method is often the naïve method**. That is, each forecast is simply equal to the last observed value, or $\hat{y}_t=y_{t−1}$. Hence, the residuals are simply equal to the difference between consecutive observations:

$$e_{t} = y_{t} - \hat{y}_{t} = y_{t} - y_{t-1}.$$

The following graph shows the Google daily closing stock price for trading days during 2015. The large jump corresponds to 17 July 2015 when the price jumped 16% due to unexpectedly strong second quarter results.

```{r}
autoplot(google_2015, Close) +
  labs(y = "$US", title = "Google daily closing stock prices in 2015")
```

These are the residuals obtained from forecasting this series using the naïve method. The large positive residual is a result of the unexpected price jump in July.

```{r}
aug <- google_2015 |>
  model(NAIVE(Close)) |>
  augment()
autoplot(aug, .innov) +
  labs(y = "$US",
       title = "Residuals from the naive method")
```

```{r}
aug |>
  ggplot(aes(x = .innov)) +
  geom_histogram() +
  labs(title = "Histogram of residuals")
```
```{r}
aug |>
  ACF(.innov) |>
  autoplot() +
  labs(title = "Residuals from the naive method")
```

These graphs show that the naïve method produces forecasts that appear to account for all available information. The mean of the residuals is close to zero and there is no significant correlation in the residuals series. The time plot of the residuals shows that the variation of the residuals stays much the same across the historical data, apart from the one outlier, and therefore the residual variance can be treated as constant. This can also be seen on the histogram of the residuals. The histogram suggests that the residuals may not be normal — the right tail seems a little too long, even when we ignore the outlier. Consequently, forecasts from this method will probably be quite good, but prediction intervals that are computed assuming a normal distribution may be inaccurate.

A **convenient shortcut** for producing these residual diagnostic graphs is the `gg_tsresiduals()` function, which will produce a time plot, ACF plot and histogram of the residuals.

```{r}
google_2015 |>
  model(NAIVE(Close)) |>
  gg_tsresiduals()
```

## Portmanteau tests for autocorrelation

In addition to looking at the ACF plot, we can also do a more formal test for autocorrelation by considering a whole set of $r_k$ values as a group, rather than treating each one separately.

Recall that $r_k$ is the autocorrelation for lag $k$. When we look at the ACF plot to see whether each spike is within the required limits, we are implicitly carrying out multiple hypothesis tests, each one with a small probability of giving a false positive. When enough of these tests are done, it is likely that at least one will give a false positive, and so we may conclude that the residuals have some remaining autocorrelation, when in fact they do not.

In order to overcome this problem, we test whether the first $l$ autocorrelations are significantly different from what would be expected from a white noise process. A test for a group of autocorrelations is called a **portmanteau test**, from a French word describing a suitcase or coat rack carrying several items of clothing.

One such test is the **Box-Pierce test**, based on the following statistic

$$Q = T \sum_{k=1}^\ell r_k^2,$$

where $l$ is the maximum lag being considered and $T$ is the number of observations. If each $r_k$ is close to zero, then $Q$ will be small. If some $r_k$ values are large (positive or negative), then $Q$ will be large. **We suggest using $l=10$ for non-seasonal data and $l=2m$ for seasonal data, where $m$ is the period of seasonality**. However, the test is not good when $l$ is large, so if these values are larger than $T/5$, then use $l=T/5$.

A related (and more accurate) test is the **Ljung-Box test**, based on

$$Q^* = T(T+2) \sum_{k=1}^\ell (T-k)^{-1}r_k^2.$$

Again, large values of $Q^*$ suggest that the autocorrelations do not come from a white noise series.

How large is too large? If the autocorrelations did come from a white noise series, then both $Q$ and $Q^*$ would have a $\chi^2$ distribution with $\ell$ degrees of freedom.

In the following code, `lag` = $\ell$

```{r}
aug |> features(.innov, box_pierce, lag = 10)
```
```{r}
aug |> features(.innov, ljung_box, lag = 10)
```

For both $Q$ and $Q^*$ the results are not significant (i.e., the $p$-values are relatively large). Thus, we can conclude that the residuals are not distinguishable from a white noise series.

An alternative simple approach that may be appropriate for forecasting the Google daily closing stock price is the `drift` method. The `tidy()` function shows the one estimated parameter, the **drift coefficient**, measuring the average daily change observed in the historical data.

```{r}
fit <- google_2015 |> model(RW(Close ~ drift()))
tidy(fit)
```

Applying the Ljung-Box test, we obtain the following result.

```{r}
augment(fit) |> features(.innov, ljung_box, lag = 10)
```

As with the naïve method, the residuals from the drift method are indistinguishable from a white noise series.

# 5.5 Distributional forecasts and predictions

## Forecast distributions

As discussed in Section 1.7, **we express the uncertainty in our forecasts using a probability distribution**. It describes the probability of observing possible future values using the fitted model. The point forecast is the mean of this distribution. **Most time series models produce normally distributed forecasts** — that is, we assume that the distribution of possible future values follows a normal distribution. We will look at a couple of alternatives to normal distributions later in this section.

## Prediction intervals

A prediction interval gives an interval within which we expect $y_t$ to lie with a specified probability. For example, assuming that distribution of future observations is normal, a 95% prediction interval for the $h$-step forecast is

$$\hat{y}_{T+h|T} \pm 1.96 \hat\sigma_h$$

where $\hat\sigma_h$ is an estimate of the standard deviation of the $h$-step forecast distribution.

More generally, a prediction interval can be written as:

$$\hat{y}_{T+h|T} \pm c\hat\sigma_h$$

where the multiplier $c$ depends on the coverage probability. In this book we usually calculate 80% intervals and 95% intervals, although any percentage may be used. Table 5.1 gives the value of $c$ for a range of coverage probabilities assuming a normal forecast distribution.

### Multipliers to be used for predictions

|Percentage|Multiplier|
|----------|----------|
|50|0.67|
|55|0.76|
|60|0.84|
|65|0.93|
|70|1.04|
|75|1.15|
|80|1.28|
|85|1.44|
|90|1.64|
|95|1.96|
|96|2.05|
|97|2.17|
|98|2.33|
|99|2.58|

The value of prediction intervals is that they express the uncertainty in the forecasts. If we only produce point forecasts, there is no way of telling how accurate the forecasts are. However, if we also produce prediction intervals, then it is clear how much uncertainty is associated with each forecast. For this reason, point forecasts can be of almost no value without the accompanying prediction intervals.

## One-step prediction intervals

When forecasting one step ahead, the standard deviation of the forecast distribution can be estimated using the standard deviation of the residuals given by

$$
\begin{equation}
  \hat{\sigma} = \sqrt{\frac{1}{T-K-M}\sum_{t=1}^T e_t^2}, \tag{5.1}
\end{equation}
$$

where $K$ is the number of parameters estimated in the forecasting method, and $M$ is the number of missing values in the residuals. (For example, $M=1$ fora naive forecast, because we can’t forecast the first observation.)

For example, consider a naïve forecast for the Google stock price data `google_2015`. The last value of the observed series is 758.88, so the forecast of the next value of the price is 758.88. The standard deviation of the residuals from the naïve method, as given by Equation (5.1), is 11.19. Hence, a 95% prediction interval for the next value of the GSP is

$$758.88 \pm 1.96(11.19) = [736.9, 780.8].$$

Similarly, an 80% prediction interval is given by

$$758.88 \pm 1.28(11.19) = [744.5, 773.2].$$
## Multi-step prediction intervals

A common feature of prediction intervals is that they usually increase in length as the forecast horizon increases. The further ahead we forecast, the more uncertainty is associated with the forecast, and thus the wider the prediction intervals. That is, $\sigma_h$ usually increases with $h$ (although there are some non-linear forecasting methods which do not have this property).

To produce a prediction interval, it is necessary to have an estimate of $\sigma_h$. As already noted, for one-step forecasts ($h=1$), Equation (5.1) provides a good estimate of the forecast standard deviation $\sigma_1$.  For multi-step forecasts, a more complicated method of calculation is required. These calculations assume that the residuals are uncorrelated.

## Benchmark methods

For the four benchmark methods, it is possible to mathematically derive the forecast standard deviation under the assumption of uncorrelated residuals. If $\hat\sigma$ denotes the standard deviation of the $h$-step forecast distribution, and $\hat\sigma$ is the residual standard deviation given by (5.1), then we can use the expressions shown in Table 5.2. Note that when $h=1$ and $T$ is large, these all give the same approximate value $\hat\sigma$.

!Benchmark method !$h$-step forecast standard deviation|
|-----------------|------------------------------------|
|Mean|$\hat\sigma_h = \hat\sigma\sqrt{1 + 1/T}$|
|Naive|$\hat\sigma_h = \hat\sigma\sqrt{h}$|
|Seasonal naive|$\hat\sigma_h = \hat\sigma\sqrt{k+1}$|
|Drift|$\hat\sigma_h = \hat\sigma\sqrt{h(1+h/(T-1))}$|

Prediction intervals can easily be computed for you when using the 1fable1 package. For example, here is the output when using the naïve method for the Google stock price.

```{r}
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  hilo()
```

The `hilo()` function converts the forecast distributions into intervals. By default, 80% and 95% prediction intervals are returned, although other options are possible via the level argument.

When plotted, the prediction intervals are shown as shaded regions, with the strength of colour indicating the probability associated with the interval. Again, 80% and 95% intervals are shown by default, with other options available via the `level` argument.

```{r}
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10) |>
  autoplot(google_2015) +
  labs(y ="$US",
       title = "Google daily closing stock price")
```

## Prediction intervals from bootstrapped residuals

When a normal distribution for the residuals is an unreasonable assumption, one alternative is to use bootstrapping, which only assumes that the residuals are uncorrelated with constant variance.

A one-step forecast error is defined as $e_t=y_t-\hat{y}_{t|t-1}$. We can re-write this as

$$y_t = \hat{y}_{t|t-1} + e_t.$$

So we can simulate the next observation of a time series using

$$y_{T+1} = \hat{y}_{T+1|T} + e_{T+1}$$

Where $\hat{y}_{t|t-1}$ is the one-step forecast and $e_{T+1}$ is an unknown future error. Assuming future errors will be similar to past errors, we can replace $e_{T+1}$ by sampling from the collection of errors we have seen in the past (i.e., the residuals). Adding the new simulated observation to our data set, we can repeat the process to obtain

$$y_{T+2} = \hat{y}_{T+2|T+1} + e_{T+2}$$

Doing this repeatedly, we obtain many possible futures. To see some of them, we can use the `generate()` function.

```{r}
fit <- google_2015 |>
  model(NAIVE(Close))
sim <- fit |> generate(h = 30,
                       times = 5,
                       bootstrap = TRUE)
sim
```

Here we have generated five possible sample paths for the next 30 trading days. The .rep variable provides a new key for the tsibble. The plot below shows the five sample paths along with the historical data.

```{r}
google_2015 |>
  ggplot(aes(x = day)) +
  geom_line(aes(y = Close)) +
  geom_line(aes(y = .sim, 
                colour = as.factor(.rep)),
            data = sim) +
  labs(title = "Google daily closing stock price",
       y = "$US") +
  guides(colour = "none")
```

Then we can compute prediction intervals by calculating percentiles of the future sample paths for each forecast horizon. The result is called a bootstrapped prediction interval. The name “bootstrap” is a reference to pulling ourselves up by our bootstraps, because the process allows us to measure future uncertainty by only using the historical data.

This is all built into the `forecast()` function so you do not need to call `generate()` directly.

```{r}
fc <- fit |> forecast(h = 30, bootstrap = TRUE)
fc
```

Notice that the forecast distribution is now represented as a simulation with 5000 sample paths. Because there is no normality assumption, the prediction intervals are not symmetric. The `.mean` column is the mean of the bootstrap samples, so it may be slightly different from the results obtained without a bootstrap.

```{r}
autoplot(fc, google_2015) +
  labs(title = "Google daily closing stock price",
       y = "$US")
```

The number of samples can be controlled using the `times` argument for `forecast()`. For example, intervals based on 1000 bootstrap samples can be sampled with:

```{r}
google_2015 |>
  model(NAIVE(Close)) |>
  forecast(h = 10, bootstrap = TRUE, times = 1000) |>
  hilo()
```

In this case, they are similar (but not identical) to the prediction intervals based on the normal distribution.





