---
title: "Chapter 8 Exponential smoothing (8.1-8.4)"
output:
  html_document:
    toc: true
    df_print: paged
  github_document:
    toc: true
---

```{r}
library(fpp3)
```

**Exponential smoothing** was proposed in the late 1950s (Brown, 1959; Holt, 1957; Winters, 1960), and has motivated some of the most successful forecasting methods. <span style="background-color:#ffffb3;">Forecasts produced using exponential smoothing methods are **weighted averages of past observations**, with the weights **decaying exponentially** as the observations get older.</span> In other words, the more recent the observation the higher the associated weight. <span style="background-color:#ffffb3;">This framework generates reliable forecasts quickly and for a wide range of time series</span>, which is a great advantage and of major importance to applications in industry.

This chapter is divided into two parts. In the first part (Sections 8.1–8.4) we present the mechanics of the most important exponential smoothing methods, and their application in forecasting time series with various characteristics. This helps us develop an intuition to how these methods work. In this setting, selecting and using a forecasting method may appear to be somewhat ad hoc. <span style="background-color:#ffffb3;">The selection of the method is generally based on recognising key components of the time series (trend and seasonal) and the way in which these enter the smoothing method (e.g., in an additive, damped or multiplicative manner).</span>

In the second part of the chapter (Sections 8.5–8.7) we present the statistical models that underlie exponential smoothing methods. These models generate identical point forecasts to the methods discussed in the first part of the chapter, but also generate prediction intervals. Furthermore, this statistical framework allows for genuine model selection between competing models.

`ETS` from `fable`: Exponential smoothing state space model

# 8.1 Simple exponential smoothing

The simplest of the exponential smoothing methods is naturally called **simple exponential smoothing (SES)**. <span style="background-color:#ffffb3;">This method is suitable for forecasting data with no clear trend or seasonal pattern.</span> For example, the data in Figure 8.1 do not display any clear trending behaviour or any seasonality. (There is a decline in the last few years, which might suggest a trend. We will consider whether a trended method would be better for this series later in this chapter.) We have already considered the naïve and the average as possible methods for forecasting such data (Section 5.2).

```{r}
algeria_economy <- global_economy |>
  filter(Country == "Algeria")
algeria_economy |>
  autoplot(Exports) +
  labs(y = "% of GDP", title = "Algerian exports")
```

Using the naïve method, all forecasts for the future are equal to the last observed value of the series,

$$\hat{y}_{T+h|T} = y_{T},$$

for $h=1,2,\dots$. Hence, the naïve method assumes that the most recent observation is the only important one, and all previous observations provide no information for the future. This can be thought of as a weighted average where all of the weight is given to the last observation.

Using the average method, all future forecasts are equal to a simple average of the observed data,

$$\hat{y}_{T+h|T} = \frac1T \sum_{t=1}^T y_t,$$

We often want something between these two extremes. For example, it may be sensible to attach larger weights to more recent observations than to observations from the distant past. This is exactly the concept behind simple exponential smoothing. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past — the smallest weights are associated with the oldest observations:

$$
\begin{equation}
  \hat{y}_{T+1|T} = \alpha y_T + \alpha(1-\alpha) y_{T-1} + \alpha(1-\alpha)^2 y_{T-2}+ \cdots,   \tag{8.1}
\end{equation}
$$

where $0\le\alpha\le1$ is the smoothing parameter. The one-step-ahead forecast for time $T+1$ is a weighted average of all of the observations in the series $y_1,\dots,y_T$. The rate at which the weights decrease is controlled by the parameter $\alpha$.

The table below shows the weights attached to observations for four different values of $\alpha$ when forecasting using simple exponential smoothing. Note that the sum of the weights even for a small value of $\alpha$ will be approximately one for any reasonable sample size.

||$\alpha=0.2$|$\alpha=0.4$|$\alpha=0.6$|$\alpha=0.8$|
|:--|--:|--:|--:|--:|
|$y_T$    |0.2000|0.4000|0.6000|0.8000|
|$y_{T-1}$|0.1600|0.2400|0.2400|0.1600|
|$y_{T-2}$|0.1280|0.1440|0.0960|0.0320|
|$y_{T-3}$|0.1024|0.0864|0.0384|0.0064|
|$y_{T-4}$|0.0819|0.5018|0.0154|0.0013|
|$y_{T-5}$|0.0655|0.0311|0.0061|0.0003|

For any $\alpha$ between 0 and 1, the weights attached to the observations decrease exponentially as we go back in time, hence the name “exponential smoothing”. <span style="background-color:#ffffb3;">If $\alpha$ is small (i.e., close to 0), more weight is given to observations from the more distant past. If $\alpha$ is large (i.e., close to 1), more weight is given to the more recent observations.</span> For the extreme case where $\alpha=1$, $\hat{y}_{T+1|T}=y_T$, and the forecasts are equal to the naïve forecasts.

We present two equivalent forms of simple exponential smoothing, each of which leads to the forecast Equation (8.1).

## Weighted average form

The forecast at time $T+1$ is equal to a weighted average between the most recent observation $y_T$ and the previous forecast $\hat{y}_{T|T−1}$

$$\hat{y}_{T+1|T} = \alpha y_T + (1-\alpha) \hat{y}_{T|T-1},$$
where <span style="background-color:#ffffb3;">$0\le\textbf{$\alpha$}\le1$ is the **smoothing parameter**.</span> Similarly, we can write the fitted values as

$$
\hat{y}_{t+1|t} = \alpha y_t + (1-\alpha) \hat{y}_{t|t-1},
$$

for $t=1,\dots,T$. (Recall that fitted values are simply one-step forecasts of the training data.)

The process has to start somewhere, so we let the first fitted value at time 1 be denoted by $\ell_0$ (which we will have to estimate). Then

$$
\begin{align*}
  \hat{y}_{2|1} &= \alpha y_1 + (1-\alpha) \ell_0\\
  \hat{y}_{3|2} &= \alpha y_2 + (1-\alpha) \hat{y}_{2|1}\\
  \hat{y}_{4|3} &= \alpha y_3 + (1-\alpha) \hat{y}_{3|2}\\
  \vdots\\
  \hat{y}_{T|T-1} &= \alpha y_{T-1} + (1-\alpha) \hat{y}_{T-1|T-2}\\
  \hat{y}_{T+1|T} &= \alpha y_T + (1-\alpha) \hat{y}_{T|T-1}.
\end{align*}
$$

Substituting each equation into the following equation, we obtain

$$
\begin{align*}
  \hat{y}_{3|2}   & = \alpha y_2 + (1-\alpha) \left[\alpha y_1 + (1-\alpha) \ell_0\right]              \\
                 & = \alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0                          \\
  \hat{y}_{4|3}   & = \alpha y_3 + (1-\alpha) [\alpha y_2 + \alpha(1-\alpha) y_1 + (1-\alpha)^2 \ell_0]\\
                 & = \alpha y_3 + \alpha(1-\alpha) y_2 + \alpha(1-\alpha)^2 y_1 + (1-\alpha)^3 \ell_0 \\
                 & ~~\vdots                                                                           \\
  \hat{y}_{T+1|T} & =  \sum_{j=0}^{T-1} \alpha(1-\alpha)^j y_{T-j} + (1-\alpha)^T \ell_{0}.
\end{align*}
$$

The last term becomes tiny for large $T$. So, the weighted average form leads to the same forecast Equation (8.1).

## Component form

An alternative representation is the component form. <span style="background-color:#ffffb3;">For simple exponential smoothing, the only component included is the level, $\ell_t$</span>. (Other methods which are considered later in this chapter may also include a trend $b_t$ and a seasonal component $s_t$.) Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each of the components included in the method. The component form of simple exponential smoothing is given by:

$$
\begin{align*}
  \text{Forecast equation}  && \hat{y}_{t+h|t} & = \ell_{t}\\
  \text{Smoothing equation} && \ell_{t}        & = \alpha y_{t} + (1 - \alpha)\ell_{t-1},
\end{align*}
$$

where $\ell_t$ is the level (or the smoothed value) of the series at time $t$ . Setting $h=1$ gives the fitted values, while setting $t=T$ gives the true forecasts beyond the training data.

<span style="background-color:#ffffb3;">The forecast equation shows that the forecast value at time $t+1$ is the estimated level at time $t$</span>. The smoothing equation for the level (usually referred to as the **level equation**) gives the estimated level of the series at each period $t$.

If we replace $\ell_t$ with $\hat{y}_{t+1|t}$ and $\ell_{t-1}$ with $\hat{y}_{t|t−1}$ in the smoothing equation, we will recover the weighted average form of simple exponential smoothing.

The component form of simple exponential smoothing is not particularly useful on its own, but it will be the easiest form to use when we start adding other components.

## Flat forecasts

Simple exponential smoothing has a “flat” forecast function:

$$\hat{y}_{T+h|T} = \hat{y}_{T+1|T}=\ell_T, \qquad h=2,3,\dots.$$

That is, all forecasts take the same value, equal to the last level component. Remember that these forecasts will only be suitable if the time series has no trend or seasonal component.

## Optimisation

The application of every exponential smoothing method requires the smoothing parameters and the initial values to be chosen. In particular, for simple exponential smoothing, we need to select the values of $\alpha$ and $\ell_0$. All forecasts can be computed from the data once we know those values. For the methods that follow there is usually more than one smoothing parameter and more than one initial component to be chosen.

In some cases, the smoothing parameters may be chosen in a subjective manner — the forecaster specifies the value of the smoothing parameters based on previous experience. However, a more reliable and objective way to obtain values for the unknown parameters is to estimate them from the observed data.

In Section 7.2, we estimated the coefficients of a regression model by minimising the sum of the squared residuals (usually known as SSE or “sum of squared errors”). Similarly, <span style="background-color:#ffffb3;">the unknown parameters and the initial values for any exponential smoothing method can be estimated by minimising the SSE</span>. The residuals are specified as $e_t=y_t−\hat{y}_{t|t−1}$ for $t=1,\dots,T$ . Hence, we find the values of the unknown parameters and the initial values that minimise

$$
\begin{equation}
\text{SSE}=\sum_{t=1}^T(y_t - \hat{y}_{t|t-1})^2=\sum_{t=1}^Te_t^2. \tag{8.2}
\end{equation}
$$

<span style="background-color:#ffffb3;">Unlike the regression case (where we have formulas which return the values of the regression coefficients that minimise the SSE), this involves a non-linear minimisation problem, and we need to use an optimisation tool to solve it.</span>

## Example: Algerian exports

In this example, simple exponential smoothing is applied to forecast exports of goods and services from Algeria.

```{r}
fit <- algeria_economy |>
  model(ETS(Exports ~ error("A") + trend("N") + season("N")))
fc <- fit |>
  forecast(h = 5)
```

This gives parameter estimates $\hat{\alpha}=0.84$ and $\hat{\ell}_0=39.5$, obtained by minimising SSE over periods $t=1,2,\dots,58$, subject to the restriction that $0\le\alpha\le1$.

In Table 8.1 we demonstrate the calculation using these parameters. The second last column shows the estimated level for times $t=0$ to $t=58$; the last few rows of the last column show the forecasts for $h=$1 to 5-steps ahead.

|Year|Time|Observation|Level|Forecast|
|:--|:--|:--|:--|:--|:--|
|   |$t$|$y_t$|$\ell_t$|$\hat{y}_{t\|t-1}$|
|1959|0||39.54||
|1960|1|39.04|39.12|39.54|
|1961	|2	|46.24	|45.10	|39.12|
|1962	|3	|19.79	|23.84	|45.10|
|1963	|4	|24.68	|24.55	|23.84|
|1964	|5	|25.08	|25.00	|24.55|
|1965	|6	|22.60	|22.99	|25.00|
|1966	|7	|25.99	|25.51	|22.99|
|1967	|8	|23.43	|23.77	|25.51|
|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|
|2014	|55	|30.22	|30.80	|33.85|
|2015	|56	|23.17	|24.39	|30.80|
|2016	|57	|20.86	|21.43	|24.39|
|2017	|58	|22.64	|22.44	|21.43|
|     |$h$|||$\hat{y}_{T+h\|T}$|
|2018	|1	|||		22.44|
|2019	|2	|||		22.44|
|2020	|3	|||		22.44|
|2021	|4	|||		22.44|
|2022	|5	|||		22.44|

The black line in Figure 8.2 shows the data, which has a changing level over time.

```{r}
fc |>
  autoplot(algeria_economy) +
  geom_line(aes(y = .fitted),
            col = "#D55E00",
            data = augment(fit)) +
  labs(y = "% of GDP", title = "Algerian exports") +
  guides(colour = "none")
```

The forecasts for the period 2018–2022 are plotted in Figure 8.2. Also plotted are one-step-ahead fitted values alongside the data over the period 1960–2017. <span style="background-color:#ffffb3;">The large value of $\alpha$ in this example is reflected in the large adjustment that takes place in the estimated level $\ell_t$ at each time. A smaller value of $\alpha$ would lead to smaller changes over time, and so the series of fitted values would be smoother.<\span>

The prediction intervals shown here are calculated using the methods described in Section 8.7. The prediction intervals show that there is considerable uncertainty in the future exports over the five-year forecast period. <span style="background-color:#ffffb3;">So interpreting the point forecasts without accounting for the large uncertainty can be very misleading.<\span>

# 8.2 Methods with trend

## Holt's linear trend method

Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend):

$$
\begin{align*}
  \text{Forecast equation}&& \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} \\
  \text{Level equation}   && \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  \text{Trend equation}   && b_{t}    &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)b_{t-1},
\end{align*}
$$

where $\ell_t$ denotes an estimate of the level of the series at time $t,b_t$ denotes an estimate of the trend (slope) of the series at time $t,\alpha$ is the smoothing parameter for the level, $0\le\alpha\le1$, and $\beta^*$ is the smoothing parameter for the trend, $0\le\beta^*\le1$. (We denote this as $\beta^*$ instead of $\beta$ for reasons that will be explained in Section 8.5.)

As with simple exponential smoothing, the level equation here shows that $\ell_t$ is a weighted average of observation $y_t$ and the one-step-ahead training forecast for time $t$ , here given by $\ell_{t−1}+b_{t−1}$. The trend equation shows that $b_t$ is a weighted average of the estimated trend at time $t$ based on $\ell_t−\ell_{t−1}$ and $b_{t−1}$, the previous estimate of the trend.

The forecast function is no longer flat but trending. The $h$-step-ahead forecast is equal to the last estimated level plus $h$ times the last estimated trend value. Hence the forecasts are a linear function of $h$.

## Example: Australian population 

```{r}
aus_economy <- global_economy |>
  filter(Code == "AUS") |>
  mutate(Pop = Population / 1e6)
autoplot(aus_economy, Pop) +
  labs(y = "Millions", title = "Australian population")
```

Figure 8.3 shows Australia’s annual population from 1960 to 2017. We will apply Holt’s method to this series. <span style="background-color:#ffffb3;">The smoothing parameters, $\alpha$ and $\beta^*$ , and the initial values $\ell_0$ and $b_0$ are estimated by minimising the SSE for the one-step training errors </span> as in Section 8.1.

```{r}
fit <- aus_economy |>
  model(
    AAN = ETS(Pop ~ error("A") + trend("A") + season("N"))
  )
fc <- fit |> forecast(h = 10)
fc
```

The estimated smoothing coefficient for the level is $\hat{\alpha}=0.9999$ . The very high value shows that the level changes rapidly in order to capture the highly trended series. The estimated smoothing coefficient for the slope is $\hat{\beta}^*=0.3267$.This is relatively large suggesting that the trend also changes often (even if the changes are slight).

In Table 8.2 we use these values to demonstrate the application of Holt’s method.

|Year|Time|Observation|Level|Slope|Forecast|
|:--|:--|:--|:--|:--|:--|:--|
|     |$t$|$y_t$ |$\ell_t$|    |$\hat{y}_{t\|t-1}$|
|1959	|0	|      |10.05   |0.22|     |
|1960	|1	|10.28 |10.28	  |0.22|10.28|
|1961	|2	|10.48 |10.48	  |0.22|	10.50|
|1962	|3	|10.74	|10.74	|0.23|	10.70|
|1964	|5	|11.17	|11.17	|0.22|	11.17|
|1965	|6	|11.39	|11.39	|0.22|	11.39|
|1966	|7	|11.65	|11.65	|0.23|	11.61|
|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|
|2014	|55	|23.50	|23.50	|0.37	|23.52|
|2015	|56	|23.85	|23.85	|0.36	|23.87|
|2016	|57	|24.21	|24.21	|0.36	|24.21|
|2017	|58	|24.60	|24.60	|0.37	|24.57|
|     |$h$|       |       |     |$\hat{y}_{T+h\|T}$|
|2018	|1	|			  |       |     |24.97|
|2019	|2	|			  |       |     |25.34|
|2020	|3	|			  |       |     |25.71|
|2021	|4	|			  |       |     |26.07|
|2022	|5	|			  |       |     |26.44|
|2023	|6	|			  |       |     |27.18|
|2025	|8	|			  |       |     |27.55|
|2026	|9	|			  |       |     |27.92|
|2027	|10	|			  |       |     |28.29|

## Damped trend methods

The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future. Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Motivated by this observation, Gardner & McKenzie (1985) introduced a parameter that “dampens” the trend to a flat line some time in the future. <span style="background-color:#ffffb3;">Methods that include a damped trend have proven to be very successful, and are arguably the most popular individual methods when forecasts are required automatically for many series.<\span>

In conjunction with the smoothing parameters $\alpha$ and $\beta^*$ (with values between 0 and 1 as in Holt’s method), this method also includes a damping parameter $0\lt\phi\lt\1$:

$$
\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t} \\
  \ell_{t} &= \alpha y_{t} + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 -\beta^*)\phi b_{t-1}.
\end{align*}
$$

If $\phi=1$ , the method is identical to Holt’s linear method. For values between $0$ and $1$,$\phi$ dampens the trend so that it approaches a constant some time in the future. In fact, the forecasts converge to $\ell_T+\phi b_T/(1−\phi)$ as $h\to\infty$ for any value $0\lt\phi\lt1$. <span style="background-color:#ffffb3;">This means that short-run forecasts are trended while long-run forecasts are constant.</span>

In practice, $\phi$ is rarely less than 0.8 as the damping has a very strong effect for smaller values. Values of $\phi$ close to 1 will mean that a damped model is not able to be distinguished from a non-damped model. <span style="background-color:#ffffb3;">For these reasons, we usually restrict $\phi$ to a minimum of 0.8 and a maximum of 0.98.</span>

## Example: Australian Population



```{r}
aus_economy |>
  model(
    `Holt's method` = ETS(Pop ~ error("A") +
                       trend("A") + season("N")),
    `Damped Holt's method` = ETS(Pop ~ error("A") +
                       trend("Ad", phi = 0.9) + season("N"))
  ) |>
  forecast(h = 15) |>
  autoplot(aus_economy, level = NULL) +
  labs(title = "Australian population",
       y = "Millions") +
  guides(colour = guide_legend(title = "Forecast"))
```

Figure 8.4 shows the forecasts for years 2018–2032 generated from Holt’s linear trend method and the damped trend method.

We have set the damping parameter to a relatively low number $(\phi=0.90)$ to exaggerate the effect of damping for comparison. Usually, we would estimate $\phi$ along with the other parameters. We have also used a rather large forecast horizon $(h=15)$ to highlight the difference between a damped trend and a linear trend.

## Example: Internet useage

In this example, we compare the forecasting performance of the three exponential smoothing methods that we have considered so far in forecasting the number of users connected to the internet via a server. The data is observed over 100 minutes and is shown in Figure 8.5.

```{r}
www_usage <- as_tsibble(WWWusage)
www_usage |> autoplot(value) +
  labs(x = "Minute",
       y = "Number of users",
       title = "Internet usage per minute")
```

We will use time series cross-validation to compare the one-step forecast accuracy of the three methods.

```{r}
www_usage |>
  stretch_tsibble(.init = 10) |>
  model(
    SES = ETS(value ~ error("A") + trend("N") + season("N")),
    Holt = ETS(value ~ error("A") + trend("A") + season("N")),
    Damped = ETS(value ~ error("A") + trend("Ad") +
                   season("N"))
  ) |>
  forecast(h = 1) |>
  accuracy(www_usage)
```

Damped Holt’s method is best whether you compare MAE or RMSE values. So we will proceed with using the damped Holt’s method and apply it to the whole data set to get forecasts for future minutes.

```{r}
fit <- www_usage |>
  model(
    Damped = ETS(value ~ error("A") + trend("Ad") + season("N"))
  )
tidy(fit)
```

The smoothing parameter for the slope is estimated to be almost one, indicating that the trend changes to mostly reflect the slope between the last two minutes of internet usage. The value of $\alpha$ is very close to one, showing that the level reacts strongly to each new observation.

```{r}
fit |>
  forecast(h = 10) |>
  autoplot(www_usage) +
  labs(x="Minute", y="Number of users",
       title = "Internet usage per minute")
```

The resulting forecasts look sensible with decreasing trend, which flattens out due to the low value of the damping parameter (0.815), and relatively wide prediction intervals reflecting the variation in the historical data. The prediction intervals are calculated using the methods described in Section 8.7.

In this example, the process of selecting a method was relatively easy as both MSE and MAE comparisons suggested the same method (damped Holt’s). However, sometimes different accuracy measures will suggest different forecasting methods, and then a decision is required as to which forecasting method we prefer to use. As forecasting tasks can vary by many dimensions (length of forecast horizon, size of test set, forecast error measures, frequency of data, etc.), it is unlikely that one method will be better than all others for all forecasting scenarios. What we require from a forecasting method are consistently sensible forecasts, and these should be frequently evaluated against the task at hand.

# 8.3 Methods with seasonality

Holt (1957) and Winters (1960) extended Holt’s method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations — one for the level $\ell_t$, one for the trend $b_t$, and one for the seasonal component $s_t$, with corresponding smoothing parameters $\alpha$, $\beta^*$ and $\gamma$. We use $m$ to denote the period of the seasonality, i.e., the number of seasons in a year. For example, for quarterly data $m=4$, and for monthly data $m=12$.

There are two variations to this method that differ in the nature of the seasonal component. <span style="background-color:#ffffb3;">The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series.</span> With the additive method, the seasonal component is expressed in absolute terms in the scale of the observed series, and in the level equation the series is seasonally adjusted by subtracting the seasonal component. Within each year, the seasonal component will add up to approximately zero. With the multiplicative method, the seasonal component is expressed in relative terms (percentages), and the series is seasonally adjusted by dividing through by the seasonal component. Within each year, the seasonal component will sum up to approximately $m$.

## Holt-Winters' additive method

$$
\begin{align*}
  \hat{y}_{t+h|t} &= \ell_{t} + hb_{t} + s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha(y_{t} - s_{t-m}) + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)b_{t-1}\\
  s_{t} &= \gamma (y_{t}-\ell_{t-1}-b_{t-1}) + (1-\gamma)s_{t-m},
\end{align*}
$$

where $k$ is the integer part of $(h−1)/m$, which ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample. The level equation shows a weighted average between the seasonally adjusted observation $(y_t−s_{t−m})$ and the non-seasonal forecast $(\ell_{t−1}+b_{t−1})$ for time $t$. The trend equation is identical to Holt’s linear method. The seasonal equation shows a weighted average between the current seasonal index, $(y_t−\ell_{t−1}−b_{t−1})$, and the seasonal index of the same season last year (i.e., $m$ time periods ago).

The equation for the seasonal component is often expressed as

$$
s_{t} = \gamma^* (y_{t}-\ell_{t})+ (1-\gamma^*)s_{t-m}.
$$

If we substitute $\ell_t$ from the smoothing equation for the level of the component form above, we get

$$
s_{t} = \gamma^*(1-\alpha) (y_{t}-\ell_{t-1}-b_{t-1})+ [1-\gamma^*(1-\alpha)]s_{t-m},
$$

If we substitute $\ell_t$ from the smoothing equation for the level of the component form above, we get

$$
s_{t} = \gamma^*(1-\alpha) (y_{t}-\ell_{t-1}-b_{t-1})+ [1-\gamma^*(1-\alpha)]s_{t-m},
$$

which is identical to the smoothing equation for the seasonal component we specify here, with $\gamma=\gamma^∗(1−\alpha)$. The usual parameter restriction is $0\le\gamma^*\le1$ , which translates to $0\le\gamma\le1-\alpha$.


## Holt-Winters' multiplicative method

$$
\begin{align*}
  \hat{y}_{t+h|t} &= (\ell_{t} + hb_{t})s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha \frac{y_{t}}{s_{t-m}} + (1 - \alpha)(\ell_{t-1} + b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t}-\ell_{t-1}) + (1 - \beta^*)b_{t-1}                \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}
$$

## Example: Deomestic overnight trips in Australia

We apply Holt-Winters’ method with both additive and multiplicative seasonality15 to forecast quarterly visitor nights in Australia spent by domestic tourists. Figure 8.7 shows the data from 1998–2017, and the forecasts for 2018–2020. The data show an obvious seasonal pattern, with peaks observed in the March quarter of each year, corresponding to the Australian summer.

```{r}
aus_holidays <- tourism |>
  filter(Purpose == "Holiday") |>
  summarise(Trips = sum(Trips)/1e3)
fit <- aus_holidays |>
  model(
    additive = ETS(Trips ~ error("A") + trend("A") +
                                                season("A")),
    multiplicative = ETS(Trips ~ error("M") + trend("A") +
                                                season("M"))
  )
fc <- fit |> forecast(h = "3 years")
fc |>
  autoplot(aus_holidays, level = NULL) +
  labs(title="Australian domestic tourism",
       y="Overnight trips (millions)") +
  guides(colour = guide_legend(title = "Forecast"))
```

```{r}
tidy(fit)
```


Table 8.3: Applying Holt-Winters’ method with additive seasonality for forecasting domestic tourism in Australia. Notice that the additive seasonal component sums to approximately zero. The smoothing parameters are $\alpha=0.2620$, $\beta^*=0.1646$, $\gamma=0.0001$ and $RMSE=0.4169$.

|Quarter|Time|Observation|Level|Slope|Season|Forecast|
|:--|:--|:--|:--|:--|:--|:--|:--|
|     |$t$|$y_t$ |$\ell_t$|$b_t$|$s_t$|$\hat{y}_{t\|t-1}$|
|1997 Q1|	0| | | |	1.5	||
|1997 Q2|	1| | | |-0.3	||
|1997 Q3|	2|	|	|	|-0.7 ||	
|1997 Q4|	3|	|9.8|	0.0	|-0.5	||
|1998 Q1|	4|11.8|9.9|0.0|1.5|11.3|
|1998 Q2|	5| 9.3|	9.9| 0.0|-0.3|9.7|
|1998 Q3|	6| 8.6|	9.7|-0.0|-0.7|9.2|
|1998 Q4|	7| 9.3| 9.8| 0.0|-0.5|9.2|
|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|
|2017 Q1|80|12.4|10.9| 0.1| 1.5|12.3|
|2017 Q2|81|10.5|10.9|0.1	|-0.3|10.7|
|2017 Q3|82|10.5|11.0|0.1	|-0.7|10.3|
|2017 Q4|83|11.2|11.3|0.1	|-0.5|10.6|
||$h$|||||$\hat{y}_{T+h\vert T}$|
|2018 Q1	|1					|||||12.9|
|2018 Q2	|2					|||||11.2|
|2018 Q3	|3					|||||11.0|
|2018 Q4	|4					|||||11.2|
|2019 Q1	|5					|||||13.4|
|2019 Q2	|6					|||||11.7|
|2019 Q3	|7					|||||11.5|
|2019 Q4	|8					|||||11.7|
|2020 Q1	|9					|||||13.9|
|2020 Q2	|10					|||||12.2|
|2020 Q3	|11					|||||11.9|
|2020 Q4	|12					|||||12.2|

Table 8.4: Applying Holt-Winters’ method with multiplicative seasonality for forecasting domestic tourism in Australia. Notice that the multiplicative seasonal component sums to approximately $m=4$. The smoothing parameters are $\alpha=0.2237$, $\beta^*=0.1360$, $\gamma=0.0001$ and $RMSE=0.4122$.

|Quarter|Time|Observation|Level|Slope|Season|Forecast|
|:--|:--|:--|:--|:--|:--|:--|:--|
|     |$t$|$y_t$ |$\ell_t$|$b_t$|$s_t$|$\hat{y}_{t\|t-1}$|
|1997 Q1|	0| | | |	1.2	||
|1997 Q2|	1| | | |1.0	||
|1997 Q3|	2|	|	|	|0.9 ||	
|1997 Q4|	3|	|10.0|	-0.0	|0.9	||
|1998 Q1|	4|11.8|10.0|-0.0|1.2|11.6|
|1998 Q2|	5| 9.3|	9.9|-0.0|1.0|9.7|
|1998 Q3|	6| 8.6|	9.8|-0.0|0.9|9.2|
|1998 Q4|	7| 9.3| 9.8|-0.0|0.9|9.2|
|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|$\vdots$|

|2017 Q1|80|12.4|10.8|0.1|1.2|12.6|
|2017 Q2|81|10.5|10.9|0.1|1.0|10.6|
|2017 Q3|82|10.5|11.1|0.1|0.9|10.2|
|2017 Q4|83|11.2|11.3|0.1|0.9|10.5|
||$h$|||||$\hat{y}_{T+h\vert T}$|
|2018 Q1	|1					|||||13.3|
|2018 Q2	|2					|||||11.2|
|2018 Q3	|3					|||||10.8|
|2018 Q4	|4					|||||11.1|
|2019 Q1	|5					|||||13.8|
|2019 Q2	|6					|||||11.7|
|2019 Q3	|7					|||||11.3|
|2019 Q4	|8					|||||11.6|
|2020 Q1	|9					|||||14.4|
|2020 Q2	|10					|||||12.2|
|2020 Q3	|11					|||||11.7|
|2020 Q4	|12					|||||12.1|

The applications of both methods (with additive and multiplicative seasonality) are presented in Tables 8.3 and 8.4 respectively. Because both methods have exactly the same number of parameters to estimate, we can compare the training RMSE from both models. In this case, the method with multiplicative seasonality fits the data slightly better.

The estimated components for both models are plotted in Figure 8.8. The small value of $\gamma$ for the multiplicative model means that the seasonal component hardly changes over time. The small value of $\beta^*$ means the slope component hardly changes over time (compare the vertical scales of the slope and level components).

<img src="https://otexts.com/fpp3/fpp_files/figure-html/fig-7-LevelTrendSeas-1.png" />

## Holt-Winters' damped method

Damping is possible with both additive and multiplicative Holt-Winters’ methods. A method that often provides accurate and robust forecasts for seasonal data is the Holt-Winters method with a damped trend and multiplicative seasonality:

$$
\begin{align*}
  \hat{y}_{t+h|t} &= \left[\ell_{t} + (\phi+\phi^2 + \dots + \phi^{h})b_{t}\right]s_{t+h-m(k+1)} \\
  \ell_{t} &= \alpha(y_{t} / s_{t-m}) + (1 - \alpha)(\ell_{t-1} + \phi b_{t-1})\\
  b_{t} &= \beta^*(\ell_{t} - \ell_{t-1}) + (1 - \beta^*)\phi b_{t-1}             \\
  s_{t} &= \gamma \frac{y_{t}}{(\ell_{t-1} + \phi b_{t-1})} + (1 - \gamma)s_{t-m}.
\end{align*}
$$

## Example: Holt-Winters' method with daily data

The Holt-Winters method can also be used for daily type of data, where the seasonal period is $m=7$, and the appropriate unit of time for $h$ is in days. Here we forecast pedestrian traffic at a busy Melbourne train station in July 2016.

```{r}
sth_cross_ped <- pedestrian |>
  filter(Date >= "2016-07-01",
         Sensor == "Southern Cross Station") |>
  index_by(Date) |>
  summarise(Count = sum(Count)/1000)
```
```{r}
sth_cross_ped |>
  filter(Date <= "2016-07-31") |>
  model(
    hw = ETS(Count ~ error("M") + trend("Ad") + season("M"))
  ) |>
  forecast(h = "2 weeks") |>
  autoplot(sth_cross_ped |> filter(Date <= "2016-08-14")) +
  labs(title = "Daily traffic: Southern Cross Station",
       y = "Pedestrians ('000)")
```

Clearly the model has identified the weekly seasonal pattern and the increasing trend at the end of the data, and the forecasts are a close match to the test data.

## 8.4 A taxonomy of exponential smoothing methods

Exponential smoothing methods are not restricted to those we have presented so far. By considering variations in the combinations of the trend and seasonal components, nine exponential smoothing methods are possible, listed in Table 8.5. Each method is labelled by a pair of letters (T,S) defining the type of ‘Trend’ and ‘Seasonal’ components. For example, $(A,M)$ is the method with an additive trend and multiplicative seasonality; $(A_d,N)$ is the method with damped trend and no seasonality; and so on.

|Trend Component|Seasonal|Component||
|---|---|---|---|---|
|               |N	|A	|M|
|         |(None)	|(Additive)	|(Multiplicative)|
|$N$ (None)	|(N,N)	|(N,A)	|(N,M)|
|$A$ (Additive)	|(A,N)	|(A,A)	|(A,M)|
|$A_d$ (Additive damped)	|$(A_d,N)$|$(A_d,A)$|$(A_d,M)$|

Some of these methods we have already seen using other names:

|Shorthand|Method|
|:--|:--|
|$(N,N)$|Simple exponential smoothing|
|$(A,N)$|Holt’s linear method|
|$(A_d,N)$	|Additive damped trend method|
|$(A,A)$	|Additive Holt-Winters’ method|
|$(A,M)$	|Multiplicative Holt-Winters’ method|
|$(A_d,M)$	|Holt-Winters’ damped method|

This type of classification was first proposed by Pegels (1969), who also included a method with a multiplicative trend. It was later extended by Gardner (1985) to include methods with an additive damped trend and by J. W. Taylor (2003) to include methods with a multiplicative damped trend. <span style="background-color:#ffffb3;">We do not consider the multiplicative trend methods in this book as they tend to produce poor forecasts.</span> See Hyndman et al. (2008) for a more thorough discussion of all exponential smoothing methods.

Table 8.6 gives the recursive formulas for applying the nine exponential smoothing methods in Table 8.5. Each cell includes the forecast equation for generating $h$-step-ahead forecasts, and the smoothing equations for applying the method.


Table 8.6: Formulas for recursive calculations and point forecasts. In each case, $\ell_t$ denotes the series level at time $t$, $b_t$ denotes the slope at time $t$, $s_t$ denotes the seasonal component of the series at time $t$, and $m$ denotes the number of seasons in a year; $\alpha$, $\beta^*$, $\gamma$ and $\phi$ are smoothing parameters, $\phi_h=\phi+\phi^2+\dots+\phi^h$, and $k$ is the integer part of $(h−1)/m$.

<img src="https://otexts.com/fpp3/figs/pegelstable-1.png" />
