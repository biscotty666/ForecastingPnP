---
title: "Chapter 7 Time series regression"
output:
  html_document:
    toc: true
    df_print: paged
  github_document:
    toc: true
---

In this chapter we discuss regression models. The basic concept is that we forecast the time series of interest $y$ assuming that it has a linear relationship with other time series $x$.

**Forecast variable $y$**:  aka regressand, dependent or explained variable. 

**Predictor variables $x$** aka regressors, independent or explanatory variables.

```{r}
library(fpp3)
library(feasts)
```

# 7.1 The linear model

## Simple linear regression

$$y_t = \beta_0 + \beta_1 x_t + \varepsilon_t$$

$\beta_0$: $y$-intercept, represents the predicted value of $y$ when $x=0$

$\beta_1$: slope, represents the average predicted change in $y$ resulting from a one unit increase in $x$.

<img src="https://otexts.com/fpp3/fpp_files/figure-html/SLRpop1-1.png" />

We can think of each observation $y_t$ as consisting of the systematic or explained part of the model, $\beta_0+\beta_1x_t$ and the random “error” $\epsilon_t$. The “error” term does not imply a mistake, but a deviation from the underlying straight line model. **$\epsilon_t$ captures anything that may affect $y_t$ other than $x_t$.**

## Example: US consumption of expenditure

```{r}
us_change
```

```{r}
us_change |>
  pivot_longer(c(Consumption, Income), names_to = "Series") |>
  autoplot(value) +
  labs(y = "% change")
```
This shows time series of quarterly percentage changes (growth rates) of real personal consumption expenditure, $y$, and real personal disposable income, $x$, for the US from 1970 Q1 to 2019 Q2.

```{r}
us_change |>
  ggplot(aes(x = Income, y = Consumption)) +
  labs(y = "Consumption (quarterly % change)",
       x = "Income (quarterly % change)") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```


This is a scatter plot of consumption changes against income changes along with the estimated regression line

$$\hat{y}_t=0.54 + 0.27x_t.$$

$\hat{y}$: value of $y$ predicted by the model.

The equation is estimated using the `TSLM()` function and shown with **`report()`**

```{r}
us_change |>
  model(TSLM(Consumption ~ Income)) |>
  report()
```

The fitted line has a positive slope, reflecting the positive relationship between income and consumption. The slope coefficient shows that a one unit increase in $x$ (a 1% increase in personal disposable income) results on average in 0.27 units increase in $y$ (an average increase of 0.27% in personal consumption expenditure). Alternatively the estimated equation shows that a value of 1 for $x$ (the percentage increase in personal disposable income) will result in a forecast value of $0.54+0.27×1=0.82$ for $y$ (the percentage increase in personal consumption expenditure).

**The interpretation of the intercept requires that a value of $x=0$
makes sense.** In this case when $x=0$ (i.e., when there is no change in personal disposable income since the last quarter) the predicted value of $y$ is 0.54 (i.e., an average increase in personal consumption expenditure of 0.54%). Even when $x=0$ does not make sense, the intercept is an important part of the model. Without it, the slope coefficient can be distorted unnecessarily. The intercept should always be included unless the requirement is to force the regression line “through the origin”. In what follows we assume that an intercept is always included in the model.

## Multiple linear regression

$$
\begin{equation}
  y_t = \beta_{0} + \beta_{1} x_{1,t} + \beta_{2} x_{2,t} + \cdots + \beta_{k} x_{k,t} + \varepsilon_t,
  \tag{7.1}
\end{equation}
$$

where $y$ is the variable to be forecast and $x_1,\dots,x_k$ are the $k$
predictor variables. Each of the predictor variables must be numerical. The coefficients $\beta_1, \dots, \beta_k$ measure the effect of each predictor after taking into account the effects of all the other predictors in the model. Thus, **the coefficients measure the marginal effects of the predictor variables**.

## Example: US consumption expenditure

```{r}
us_change |>
  select(-Consumption, -Income) |>
  pivot_longer(-Quarter) |>
  ggplot(aes(Quarter, value, colour = name)) +
  geom_line() +
  facet_grid(name ~ ., scales = "free_y") +
  guides(colour = "none") +
  labs(y="% change")
```


These are quarterly percentage changes in industrial production and personal savings, and quarterly changes in the unemployment rate (as this is already a percentage). Building a multiple linear regression model can potentially generate more accurate forecasts as we expect consumption expenditure to not only depend on personal income but on other predictors as well.

```{r message=FALSE}
us_change |>
  GGally::ggpairs(columns = 2:6)
```

This is a scatterplot matrix of five variables. The first column shows the relationships between the forecast variable (consumption) and each of the predictors. The scatterplots show positive relationships with income and industrial production, and negative relationships with savings and unemployment. The strength of these relationships are shown by the correlation coefficients across the first row. The remaining scatterplots and correlation coefficients show the relationships between the predictors.

## Assumptions

1. The model is a reasonable approximation to reality; that is, the relationship between the forecast variable and the predictor variables satisfies this linear equation.

2. We make the following assumptions about the errors ${\epsilon_1,\dots,\epsilon_T}$
  - they have mean zero; otherwise the forecasts will be systematically biased.
  - they are not autocorrelated; otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited.
  - they are unrelated to the predictor variables; otherwise there would be more information that should be included in the systematic part of the model.
  
It is also useful to have the errors being normally distributed with a constant variance $\sigma^2$ in order to easily produce prediction intervals.

Another important assumption in the linear regression model is that each predictor $x$ is not a random variable. If we were performing a controlled experiment in a laboratory, we could control the values of each $x$ (so they would not be random) and observe the resulting values of $y$. With observational data (including most data in business and economics), it is not possible to control the value of $x$, we simply observe it. Hence we make this an assumption.

# 7.2 Least squares estimation

In practice, of course, we have a collection of observations but we do not know the values of the coefficients $\beta_0, \beta_1, \dots, \beta_k$ . These need to be estimated from the data.

The least squares principle provides a way of choosing the coefficients effectively by minimising the sum of the squared errors. That is, we choose the values of $\beta_0, \beta_1, \dots, \beta_k$ that minimise

$$
\sum_{t=1}^T \varepsilon_t^2 = \sum_{t=1}^T (y_t -
  \beta_{0} - \beta_{1} x_{1,t} - \beta_{2} x_{2,t} - \cdots - \beta_{k} x_{k,t})^2.
$$

This is called **least squares estimation** because it gives the least value for the sum of squared errors. **Finding the best estimates of the coefficients is often called “fitting” the model to the data**, or sometimes “learning” or “training” the model. The line shown above was obtained in this way.

$\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_k$ are the **estimated coefficients**. 

`TSLM()` fits a linear regression model to time series data. It is similar to `lm()` which is widely used for linear models, but `TSLM()` provides additional facilities for handling time series.

## Example: US consumption expenditures

A multiple linear regression model for US consumption is

$$
y_t=\beta_0 + \beta_1 x_{1,t}+ \beta_2 x_{2,t}+ \beta_3 x_{3,t}+ \beta_4 x_{4,t}+\varepsilon_t,
$$

- $y$:  % change in real personal consumption expenditure
- $x_11 % change in real personal disposable income
- $x_2$ % change in industrial production
- $x_3$ % change in personal savings and
- $x_4$ change in the unemployment rate.

The following output provides information about the fitted model. The first column of Coefficients gives an estimate of each $\beta$ coefficient and the second column gives its standard error (i.e., the standard deviation which would be obtained from repeatedly estimating the $\beta$ coefficients on similar data sets). The standard error gives a measure of the uncertainty in the estimated $\beta$ coefficient.

```{r}
fit_consMR <- us_change |>
  model(tslm = TSLM(Consumption ~ Income + Production + Unemployment + Savings))
report(fit_consMR)
```

The “t value” is the ratio of an estimated $\beta$ coefficient to its standard error and the last column gives the p-value: the probability of the estimated $\beta$ coefficient being as large as it is if there was no real relationship between consumption and the corresponding predictor. This is useful when studying the effect of each predictor, but is not particularly useful for forecasting.

## Fitted values

Predictions of $y$ can be obtained by using the estimated coefficients in the regression equation and setting the error term to zero. In general we write,

$$
\begin{equation}
  \hat{y}_t = \hat\beta_{0} + \hat\beta_{1} x_{1,t} + \hat\beta_{2} x_{2,t} + \cdots + \hat\beta_{k} x_{k,t}.
  \tag{7.2}
\end{equation}
$$

Plugging in the values of $x_{1,t},\dots,x_{k,t}$ for $t=1,\dots,T$ returns predictions of $y_t$ within the training set, referred to as fitted values. Note that **these are predictions of the data used to estimate the model, not genuine forecasts of future values of $y$**.
 .
The following plots show the actual values compared to the fitted values for the percentage change in the US consumption expenditure series. The time plot shows that the fitted values follow the actual data fairly closely. This is verified by the strong positive relationship shown by the scatterplot

.
```{r}
augment(fit_consMR) |>
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Consumption, colour = "Date")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  labs(y = NULL,
       title = "Percent change in US consumption expenditure") +
  scale_colour_manual(values = c(Data = "black", Fitted = "#D55E00")) +
  guides(colour = guide_legend(title = NULL))
```

```{r}
augment(fit_consMR) |>
  ggplot(aes(x = Consumption, y = .fitted)) +
  geom_point() +
  labs(
    y = "Fitted (predicted values)",
    x = "Data (actual values)",
    title = "Percent change in US consumption expenditure"
  ) +
  geom_abline(intercept = 0, slope = 1)
```

## Goodness-of-fit

A common way to summarise how well a linear regression model fits the data is via the **coefficient of determination**, or $R^2$. This can be calculated as the square of the correlation between the observed $y$ values and the predicted $\hat{y}$ values. Alternatively, it can also be calculated as,

$$
R^2 = \frac{\sum(\hat{y}_{t} - \bar{y})^2}{\sum(y_{t}-\bar{y})^2},
$$

where the summations are over all observations. Thus, it reflects the proportion of variation in the forecast variable that is accounted for (or explained) by the regression model.

In simple linear regression, the value of $R^2$ is also equal to the square of the correlation between $y$ and $x$ (provided an intercept has been included).

If the predictions are close to the actual values, we would expect $R^2$ to be close to 1. On the other hand, if the predictions are unrelated to the actual values, then $R^2=0$ (again, assuming there is an intercept). In all cases, $R^2$ lies between 0 and 1.

The $R^2$ value is used frequently, though often incorrectly, in forecasting. The value of $R^2$ will never decrease when adding an extra predictor to the model and this can lead to over-fitting. There are no set rules for what is a good $R^2$ value, and typical values of $R^2$ depend on the type of data used. Validating a model’s forecasting performance on the test data is much better than measuring the $R^2$ value on the training data.

## Example: US consumption expenditure

Figure 7.7 plots the actual consumption expenditure values versus the fitted values. The correlation between these variables is $r=0.877$ hence $R^2=0.768$ (shown in the output above). In this case, the model does an excellent job as it explains 76.8% of the variation in the consumption data. Compare that to the $R^2$ value of 0.15 obtained from the simple regression with the same data set in Section 7.1. **Adding the three extra predictors has allowed a lot more of the variation in the consumption data to be explained.**

## Standard error of the regression

Another measure of how well the model has fitted the data is the standard deviation of the residuals, which is often known as the “residual standard error”. This is shown in the above output with the value 0.31.

It is calculated

$$
\begin{equation}
  \hat{\sigma}_e=\sqrt{\frac{1}{T-k-1}\sum_{t=1}^{T}{e_t^2}},
  \tag{7.3}
\end{equation}
$$

where $k$ is the number of predictors in the model. Notice that we divide by $T−k−1$ because we have estimated $k+1$ parameters (the intercept and a coefficient for each predictor variable) in computing the residuals.

The standard error is related to the size of the average error that the model produces. We can compare this error to the sample mean of $y$ or with the standard deviation of $y$ to gain some perspective on the accuracy of the model.

The standard error will be used when generating prediction intervals, discussed in Section 7.6.

# 7.3 Evaluating the regression model

The differences between the observed $y$ values and the corresponding fitted $\hat{y}$ values are the training-set errors or “residuals” defined as,

$$
\begin{align*}
  e_t &= y_t - \hat{y}_t \\
      &= y_t - \hat\beta_{0} - \hat\beta_{1} x_{1,t} - \hat\beta_{2} x_{2,t} - \cdots - \hat\beta_{k} x_{k,t}
\end{align*}
$$

for $t=1, \dots, T$. 

Each residual is the unpredictable component of the associated observation.

The residuals have some useful properties including the following two:

$$
\sum_{t=1}^Te_t=0 \quad \text{and} \quad
\sum_{t-1}^Tx_{k,t}e_t = 0 \quad \text{for all }k.
$$

As a result of these properties, it is clear that **the average of the residuals is zero**, and that **the correlation between the residuals and the observations for the predictor variable is also zero**. (This is not necessarily true when the intercept is omitted from the model.)

After selecting the regression variables and fitting a regression model, it is necessary to plot the residuals to check that the assumptions of the model have been satisfied. **There are a series of plots that should be produced in order to check different aspects of the fitted model and the underlying assumptions.** We will now discuss each of them in turn.

## ACF plot of residuals

With time series data, it is highly likely that the value of a variable observed in the current time period will be similar to its value in the previous period, or even the period before that, and so on. Therefore when fitting a regression model to time series data, it is common to find autocorrelation in the residuals. In this case, the estimated model violates the assumption of no autocorrelation in the errors, and our forecasts may be inefficient — there is some information left over which should be accounted for in the model in order to obtain better forecasts. **The forecasts from a model with autocorrelated errors are still unbiased, and so are not “wrong”, but they will usually have larger prediction intervals than they need to.** Therefore we should always look at an ACF plot of the residuals.

## Histogram of residuals

It is always a good idea to check whether the residuals are normally distributed. As we explained earlier, this is not essential for forecasting, but it does make the calculation of prediction intervals much easier.

### Example

Using the `gg_tsresiduals()` function introduced in Section 5.3, we can obtain all the useful residual diagnostics mentioned above.

```{r}
fit_consMR |> gg_tsresiduals()
```

```{r}
augment(fit_consMR) |>
  features(.innov, ljung_box, lag = 10)
```

The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate.

**heteroscedasticity* (also spelled heteroskedasticity) refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable that predicts it.

The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals.

The autocorrelation plot shows a significant spike at lag 7, and a significant Ljung-Box test at the 5% level. However, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have any noticeable impact on the forecasts or the prediction intervals. In Chapter 10 we discuss dynamic regression models used for better capturing information left in the residuals.

## Residual plots against predictors

We would expect the residuals to be randomly scattered without showing any systematic patterns. A simple and quick way to check this is to examine scatterplots of the residuals against each of the predictor variables. If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly. See Section 7.7 for a discussion of nonlinear regression.

It is also necessary to plot the residuals against any predictors that are not in the model. If any of these show a pattern, then the corresponding predictor may need to be added to the model (possibly in a nonlinear form).

### Example

```{r}
us_change |>
  left_join(residuals(fit_consMR), by = "Quarter") |>
  pivot_longer(Income:Unemployment,
               names_to = "regressor", values_to = "x") |>
  ggplot(aes(x = x, y = .resid)) +
  geom_point() +
  facet_wrap(. ~ regressor, scales = "free_x") +
  labs(y = "Residuals", x = "")
```

The residuals from the multiple regression model for forecasting US consumption plotted against each predictorseem to be randomly scattered. Therefore we are satisfied with these in this case.

## Residual plots against fitted values

A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be “heteroscedasticity” in the errors which means that the variance of the residuals may not be constant. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required (see Section 3.1).

### Example

```{r}
augment(fit_consMR) |>
  ggplot(aes(x = .fitted, y = .resid)) +
  geom_point() +
  labs(x = "Fitted", y = "Residuals")
```

Continuing the previous example, Figure 7.10 shows the residuals plotted against the fitted values. The random scatter suggests the errors are homoscedastic.

**Homoscedasticity** describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables and the dependent variable) is the same across all values of the independent variables. 

## Outliers and influential observations

Observations that take extreme values compared to the majority of the data are called **outliers**. Observations that have a large influence on the estimated coefficients of a regression model are called **influential observations**. **Usually, influential observations are also outliers that are extreme in the $x$ direction.**

There are formal methods for detecting outliers and influential observations that are beyond the scope of this textbook. As we suggested at the beginning of Chapter 2, becoming familiar with your data prior to performing any analysis is of vital importance. A scatter plot of $y$ against each $x$ is always a useful starting point in regression analysis, and often helps to identify unusual observations.

One source of outliers is incorrect data entry. Simple descriptive statistics of your data can identify minima and maxima that are not sensible. If such an observation is identified, and it has been recorded incorrectly, it should be corrected or removed from the sample immediately.

Outliers also occur when some observations are simply different. In this case it may not be wise for these observations to be removed. If an observation has been identified as a likely outlier, it is important to study it and analyse the possible reasons behind it. The decision to remove or retain an observation can be a challenging one (especially when outliers are influential observations). It is wise to report results both with and without the removal of such observations

### Example

Figure 7.11 highlights the effect of a single outlier when regressing US consumption on income (the example introduced in Section 7.1). In the left panel the outlier is only extreme in the direction of $y$, as the percentage change in consumption has been incorrectly recorded as -4%. The orange line is the regression line fitted to the data which includes the outlier, compared to the black line which is the line fitted to the data without the outlier. In the right panel the outlier now is also extreme in the direction of $x$ with the 4% decrease in consumption corresponding to a 6% increase in income. In this case the outlier is extremely influential as the orange line now deviates substantially from the black line.

<img src="https://otexts.com/fpp3/fpp_files/figure-html/outlier-1.png" />

## Spurious regression

More often than not, time series data are “non-stationary”; that is, the values of the time series do not fluctuate around a constant mean or with a constant variance. We will deal with time series stationarity in more detail in Chapter 9, but here we need to address the effect that non-stationary data can have on regression models.

For example, consider the two variables plotted in Figure 7.12. These appear to be related simply because they both trend upwards in the same manner. However, air passenger traffic in Australia has nothing to do with rice production in Guinea.

<img src="https://otexts.com/fpp3/fpp_files/figure-html/spurious-1.png" />

```{r}
fit <- aus_airpassengers |>
  filter(Year <= 2011) |>
  left_join(guinea_rice, by = "Year") |>
  model(TSLM(Passengers ~ Production))
report(fit)
```

```{r}
fit |> gg_tsresiduals()
```

Regressing non-stationary time series can lead to spurious regressions. The output of regressing Australian air passengers on rice production in Guinea is shown in Figure 7.13. High $R^2$ and high residual autocorrelation can be signs of spurious regression. Notice these features in the output above. We discuss the issues surrounding non-stationary data and spurious regressions in more detail in Chapter 10.

Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future.

# 7.4 Some useful predictors

There are several useful predictors that occur frequently when using regression for time series data.

## Trend

It is common for time series data to be trending. A linear trend can be modelled by simply using $x_{1,t}=t$ as a predictor,

$$y_{t}= \beta_0+\beta_1t+\varepsilon_t,$$

where $t=1,\dots,T$. A **trend variable** can be specified in the `TSLM()` function using the `trend()` special. In Section 7.7 we discuss how we can also model nonlinear trends.

## Dummy variables

So far, we have assumed that each predictor takes numerical values. But what about when a predictor is a categorical variable taking only two values (e.g., “yes” and “no”)? Such a variable might arise, for example, when forecasting daily sales and you want to take account of whether the day is a public holiday or not. So the predictor takes value “yes” on a public holiday, and “no” otherwise.

This situation can still be handled within the framework of multiple regression models by creating a “dummy variable” which takes value 1 corresponding to “yes” and 0 corresponding to “no”. **A dummy variable is also known as an “indicator variable”.**

A dummy variable can also be used to account for an outlier in the data. Rather than omit the outlier, a dummy variable removes its effect. In this case, the dummy variable takes value 1 for that observation and 0 everywhere else. An example is the case where a special event has occurred. For example when forecasting tourist arrivals to Brazil, we will need to account for the effect of the Rio de Janeiro summer Olympics in 2016.

If there are more than two categories, then the variable can be coded using several dummy variables (one fewer than the total number of categories). TSLM() will automatically handle this case if you specify a factor variable as a predictor. There is usually no need to manually create the corresponding dummy variables.

## Seasonal dummy variables

Suppose that we are forecasting daily data and we want to account for the day of the week as a predictor. Then the following dummy variables can be created.

||$d_{1,t}$|$d_{2,t}$|$d_{3,t}$|$d_{4,t}$|$d_{5,t}$|$d_{6,t}$|
|---|--:|--:|--:|--:|--:|--:|
|Monday   |      1|      0|      0|      0|      0|      0|
|Tuesday  |      0|      1|      0|      0|      0|      0|
|Wednesday|      0|      0|      1|      0|      0|      0|
|Thursday |      0|      0|      0|      1|      0|      0|
|Friday   |      0|      0|      0|      0|      1|      0|
|Saturday |      0|      0|      0|      0|      0|      1|
|Sunday   |      0|      0|      0|      0|      0|      0|
|Monday   |      1|      0|      0|      0|      0|      0|
|$\vdots$   |$\vdots$ |$\vdots$ |$\vdots$ |$\vdots$ |$\vdots$ |$\vdots$ |

Notice that only six dummy variables are needed to code seven categories. That is because the seventh category (in this case Sunday) is captured by the intercept, and is specified when the dummy variables are all set to zero.

Many beginners will try to add a seventh dummy variable for the seventh category. This is known as the “dummy variable trap”, because it will cause the regression to fail. There will be one too many parameters to estimate when an intercept is also included. **The general rule is to use one fewer dummy variables than categories.** So for quarterly data, use three dummy variables; for monthly data, use 11 dummy variables; and for daily data, use six dummy variables, and so on.

The interpretation of each of the coefficients associated with the dummy variables is that it is a measure of the effect of that category relative to the omitted category. In the above example, the coefficient of $d_{1,t}$ associated with Monday will measure the effect of Monday on the forecast variable compared to the effect of Sunday. An example of interpreting estimated dummy variable coefficients capturing the quarterly seasonality of Australian beer production follows.

The `TSLM()` function will automatically handle this situation if you specify the special `season()`.

## Example: Australian quarterly beer production.

```{r}
recent_production <- aus_production |>
  filter(year(Quarter) >= 1992)

recent_production |>
  autoplot(Beer) +
  labs(y = "Megalitres",
       title = "Australian quarterly beer production")
```

We want to forecast the value of future beer production. We can model this data using a regression model with a linear trend and quarterly dummy variables,

$$
y_{t} = \beta_{0} + \beta_{1} t + \beta_{2}d_{2,t} + \beta_3 d_{3,t} + \beta_4 d_{4,t} + \varepsilon_{t},
$$

where $d_{i,t}=1$ if $t$ is in quarter $i$ and 0 otherwise. The first quarter variable has been omitted, so the coefficients associated with the other quarters are measures of the difference between those quarters and the first quarter.

```{r}
fit_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + season()))
report(fit_beer)
```

Note that `trend()` and `season()` are not standard functions; they are “special” functions that work within the `TSLM()` model formulae.

There is an average downward trend of -0.34 megalitres per quarter. On average, the second quarter has production of 34.7 megalitres lower than the first quarter, the third quarter has production of 17.8 megalitres lower than the first quarter, and the fourth quarter has production of 72.8 megalitres higher than the first quarter.

```{r}
augment(fit_beer) |>
  ggplot(aes(x = Quarter)) +
  geom_line(aes(y = Beer, colour = "Data")) +
  geom_line(aes(y = .fitted, colour = "Fitted")) +
  scale_colour_manual(
    values = c(Data = "black", Fitted = "#D55E00")) +
  labs(y = "Megalitres",
       title = "Australian quarterly beer production") +
  guides(colour = guide_legend(title = "Series"))
```

```{r}
augment(fit_beer) |>
  ggplot(aes(x = Beer, y = .fitted,
             colour = factor(quarter(Quarter)))) +
  geom_point() +
  labs(y = "Fitted", x = "Actual values",
       title = "Australian quarterly beer production") +
  geom_abline(intercept = 0, slope = 1) +
  guides(colour = guide_legend(title = "Quarter"))
```

## Intervention variables

It is often necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect.

- **spike variable**: The effect lasts only one period. This is a dummy variable that takes value one in the period of the intervention and zero elsewhere. **A spike variable is equivalent to a dummy variable for handling an outlier.**

- **step variable**: The effect has an immediate and permanent effect, causing a level shift. **A step variable takes value zero before the intervention and one from the time of intervention onward.**

- **change of slope**: Here the intervention is handled using a piecewise linear trend; a trend that bends at the time of intervention and hence is nonlinear.

## Trading days

The number of trading days in a month can vary considerably and can have a substantial effect on sales data. To allow for this, the number of trading days in each month can be included as a predictor.

An alternative that allows for the effects of different days of the week has the following predictors:

$$
\begin{align*}
  x_{1} &= \text{number of Mondays in month;} \\
  x_{2} &= \text{number of Tuesdays in month;} \\
        & \vdots \\
  x_{7} &= \text{number of Sundays in month.}
\end{align*}
$$
## Distributed lags

It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure. Thus, the following predictors may be used.

$$
\begin{align*}
  x_{1} &= \text{advertising for previous month;} \\
  x_{2} &= \text{advertising for two months previously;} \\
        & \vdots \\
  x_{m} &= \text{advertising for }m\text{ months previously}
\end{align*}
$$

It is common to require the coefficients to decrease as the lag increases, although this is beyond the scope of this book.

## Easter

Easter differs from most holidays because it is not held on the same date each year, and its effect can last for several days. In this case, a dummy variable can be used with value one where the holiday falls in the particular time period and zero otherwise.

With monthly data, if Easter falls in March then the dummy variable takes value 1 in March, and if it falls in April the dummy variable takes value 1 in April. When Easter starts in March and finishes in April, the dummy variable is split proportionally between months.

## Fourier series

An alternative to using seasonal dummy variables, especially for long seasonal periods, is to use Fourier terms. Jean-Baptiste Fourier was a French mathematician, born in the 1700s, who showed that a series of sine and cosine terms of the right frequencies can approximate any periodic function. We can use them for seasonal patterns.

If $m$ is the seasonal period, then the first few Fourier terms are given by

$$
x_{1,t} = \sin\left(\textstyle\frac{2\pi t}{m}\right),
  x_{2,t} = \cos\left(\textstyle\frac{2\pi t}{m}\right),
  x_{3,t} = \sin\left(\textstyle\frac{4\pi t}{m}\right),
$$

$$
x_{4,t} = \cos\left(\textstyle\frac{4\pi t}{m}\right),
  x_{5,t} = \sin\left(\textstyle\frac{6\pi t}{m}\right),
  x_{6,t} = \cos\left(\textstyle\frac{6\pi t}{m}\right),
$$

and so on. If we have monthly seasonality, and we use the first 11 of these predictor variables, then we will get exactly the same forecasts as using 11 dummy variables.

With Fourier terms, we often need fewer predictors than with dummy variables, especially when $m$ is large. This makes them useful for weekly data, for example, where $m\approx52$. For short seasonal periods (e.g., quarterly data), there is little advantage in using Fourier terms over seasonal dummy variables.

These Fourier terms are produced using the `fourier()` function. For example, the Australian beer data can be modelled like this.

```{r}
fourier_beer <- recent_production |>
  model(TSLM(Beer ~ trend() + fourier(K = 2)))
report(fourier_beer)
```

The `K` argument to `fourier()` specifies how many pairs of sin and cos terms to include. The maximum allowed is $K=m/2$ where $m$ is the seasonal period. Because we have used the maximum here, the results are identical to those obtained when using seasonal dummy variables.

If only the first two Fourier terms are used ($x_{1,t}$ and $x_{2,t}$ ), the seasonal pattern will follow a simple sine wave. A regression model containing Fourier terms is often called a harmonic regression because the successive Fourier terms represent harmonics of the first two Fourier terms.

